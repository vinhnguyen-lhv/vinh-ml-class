{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Variance Reduction for Splitting on Age = 35\n",
    "\n",
    "This notebook calculates the variance reduction for a regression decision tree predicting the `CreditScore` when splitting the training data on `Age = 35`. We then discuss how variance reduction differs from information gain in classification trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c75f5-6394-468b-99b3-2f726c7bb69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def variance(values):\n",
    "    \"\"\"Compute the variance of a numpy array of values.\"\"\"\n",
    "    mean = np.mean(values)\n",
    "    return np.mean((values - mean) ** 2)\n",
    "\n",
    "# Create the training dataset\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'Age': [35, 28, 45, 31, 52, 29, 42, 33],\n",
    "    'CreditScore': [720, 650, 750, 600, 780, 630, 710, 640],\n",
    "    'Education': [16, 14, None, 12, 18, 14, 16, 12],\n",
    "    'RiskLevel': ['Low', 'High', 'Low', 'High', 'Low', 'High', 'Low', 'High']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the parent variance for CreditScore\n",
    "parent_mean = np.mean(df['CreditScore'])\n",
    "parent_variance = variance(df['CreditScore'])\n",
    "print('Parent Mean:', parent_mean)\n",
    "print('Parent Variance:', parent_variance)\n",
    "\n",
    "# Split the dataset based on Age = 35\n",
    "group_A = df[df['Age'] <= 35]  # Group A: Age <= 35\n",
    "group_B = df[df['Age'] > 35]   # Group B: Age > 35\n",
    "\n",
    "# Calculate variance for each group\n",
    "variance_A = variance(group_A['CreditScore'])\n",
    "variance_B = variance(group_B['CreditScore'])\n",
    "\n",
    "print('Group A (Age <= 35) Variance:', variance_A)\n",
    "print('Group B (Age > 35) Variance:', variance_B)\n",
    "\n",
    "# Calculate the weighted variance after the split\n",
    "n = len(df)\n",
    "weighted_variance = (len(group_A)/n) * variance_A + (len(group_B)/n) * variance_B\n",
    "print('Weighted Variance after split:', weighted_variance)\n",
    "\n",
    "# Compute the variance reduction\n",
    "variance_reduction = parent_variance - weighted_variance\n",
    "print('Variance Reduction:', variance_reduction)\n",
    "\n",
    "# Discussion:\n",
    "print(\"\\nVariance reduction minimizes the mean squared error for continuous targets, whereas information gain in classification trees measures the reduction in impurity (entropy) for categorical outcomes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "1. **Parent Node:** The variance of the CreditScore in the full dataset is computed first. In our case, the mean is 685 and the variance is 3575.\n",
    "\n",
    "2. **Splitting on Age = 35:** The dataset is divided into two groups:\n",
    "   - **Group A (Age â‰¤ 35):** Records with CreditScores [720, 650, 600, 630, 640] with a computed variance of approximately 1576.\n",
    "   - **Group B (Age > 35):** Records with CreditScores [750, 780, 710] with a computed variance of approximately 822.22.\n",
    "\n",
    "3. **Weighted Variance and Reduction:** The weighted variance after the split is approximately 1293.33, leading to a variance reduction of about 2281.67.\n",
    "\n",
    "4. **Difference from Information Gain:** Variance reduction is used for continuous target predictions (regression) and minimizes squared errors, while information gain is used in classification tasks to reduce impurity (entropy) for categorical targets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
